---
title: "Tree Canopy Cover Density"
subtitle: "[NO_TCCD_000] - Tree Canopy Cover Density indicator in forests" /
"[NO_TCCD_100 to NO_TCCD_104] - Tree Canopy Cover Density in urban areas"
format: 
  html:
    embed-resources: true
    code-fold: true
    toc: true
    toc-title: Contents
    toc-depth: 3
    smooth-scroll: true
execute: 
  cache: true
author:
  - name: Sylvie Clappe # Enter name
    email: sylvie.clappe@nina.no  # Enter email
    affiliations:
      - id: myID
        name: Norwegian Institute for Nature Research # Enter affiliations
  - name: Bálint Czúcz            #  Enter subsequent authors like this, or remove if not relevant
    email: balint.czucz@nina.no
    affiliations:
      - ref: myID               # To reuse affiliations referecen the id like this
date: July 22, 2025# Enter date 
callout-icon: false
lightbox: true
css: ../../../style.css
code-links:
      - text: Add a review
        icon: github
        href: https://github.com/NINAnor/ecRxiv
---

<!--# This is a template for how to document the indicator analyses. Make sure also to not change the order, or modify, the headers, unless you really need to. This is because it easier to read if all the indicators are presented using the same layout. If there is one header where you don't have anything to write, just leave the header as is, and don't write anything below it. If you are providing code, be careful to annotate and comment on every step in the analysis. Before starting it is recommended to fill in as much as you can in the metadata file. This file will populate the initial table in your output.-->

<!--# Load all you dependencies here -->

```{r setup}
#| include: false
library(knitr)
library(tidyverse)
library(kableExtra)
library(terra)
library(sf)
library(dplyr)
library(readxl)
library(httr2)
library(glue)
library(tictoc)
library(duckdb)
library(duckdbfs)
library(duckspatial)
library(mirai)
library(purrr)
library(stringr)
library(exactextractr)
library(carrier)
library(hdar)

knitr::opts_chunk$set(echo = TRUE)
```

```{r source}
#| echo: false
source(here::here("_common.R"))
```

```{r}
#| echo: false
meta <- readxl::read_xlsx("../metadata.xlsx")
st <- meta |>
  filter(Variable == "status") |>
  pull(Value)
version <- meta |>
  filter(Variable == "Version") |>
  pull(Value)
auth <- meta |>
  filter(Variable == "authors") |>
  pull(Value)
year <- meta |>
  filter(Variable == "yearAdded") |>
  pull(Value)
id <- meta |>
  filter(Variable == "indicatorID") |>
  pull(Value)
name <- meta |>
  filter(Variable == "indicatorName") |>
  pull(Value)
url <- meta |>
  filter(Variable == "url") |>
  pull(Value)

meta <- meta |>
  mutate(Variable = case_match(Variable,
    "indicatorID" ~ "Indicator ID" ,
    "indicatorName" ~ "Indicator Name",
    "country" ~ "Country",
    "continent" ~ "Continent",
    "ECT" ~ "Ecosystem Condition Typology Class",
    "yearAdded" ~ "Year added",
    "yearLastUpdate" ~ "Last update",
    .default = Variable
   )
  ) |>
  filter(Variable != "authors")

```

<!--# The following parts are autogenerated. Do not edit. -->

```{r}
#| echo: false
#| results: asis
status(st)
```

::: {layout-ncol="2"}



> **Recomended citation**: `r paste(auth, " ", year, ". ", name, " (ID: ", id, ") ", "v. ", version, ". ecRxiv: https://view.nina.no/ecRxiv/", sep="")`

> **Version**: `r version`

:::

```{=html}
<details>
<summary>Show metadata</summary>
```

```{r tbl-meta}
#| tbl-cap: 'Indicator metadata'
#| echo: false
#| warning: false

meta |>
  select(Variable, Value) |>
  kbl(col.names = NULL) 

```

```{=html}
</details>
```

::: {.callout-tip collapse="true"}

## Logg

<!--# Update this logg with short messages for each update -->
- 01 Jan. 1901 - Original PR
:::


<hr />

<!--# Document you work below.  -->

## 1. Summary
## 1. Summary
***What is the tree canopy cover density indicator?*** \
Tree canopy cover is proportion of ground surface that is covered by tree crowns when viewed from above. It measures how much of the land is covered by tree foliage. / 

The tree canopy cover density is the density of tree canopy within a given area.


***How to measure air quality in an urban area?*** \
The tree canopy cover density is measured by the tree canopy cover that is visible from satellite imagery within a given area. There are 2 major steps in calculating this indicator: /

1. Identify the total area of interest (forest and urban areas).
2. Within this area, extract the percentage of tree canopy cover density.

***How to use and interpret this indicator?*** \
Although there is just one way of defining a forest area, there are several ways to define what an urban area is. That is why, in this document, we present four variations of the tree canopy cover density for urban ecosystems based on different definitions of the urban areas. We also present different spatial breakdowns of the two indicators: national and municipal levels. TCCD_000 is the tree canopy cover density indicator in forest ecosystems. It can be used for EU national reporting as it corresponds to the description given in the guidance of the European Commission. Its definition is ecologically consistent and follows the principles of the System of Environmental Economic Accounting - Ecosystem Accounting (SEEA-EA; United Nations, 2024). TCCD_100 to TCCD_104 are the different variations of the tree canopy cover density for urban ecosystems. We recommend to use TCCD_101 to TCCD_104 as they are the most aligned with the SEEA-EA principles. It however does not follow the official guidance from the European Commission. If users need to report this indicator in an official EU reporting, we recommend to use TCCD_100 instead.

<!--# 

With a maximum of 300 words, describe the indicator in general terms as you would to a non-expert. Think of this as a kind of commmon language summary. It is a good idea to include a bullet point list of the spesific steps in the workflow. Include a mention of the following aspects:

What does the metric represent?
Why is this relevant for describing ecosystem condition in this ecosystem?
What are the main anthropogenig impact factors?
What kind of data is used? 
Shortly, how is the data customized (modified, estimated, integarted) to fit its purpuse as an indicator?
What is the current status of  the metric (can it be used or is it still in development)?
How should the metric be used and interpretted, and how should it not be used/interpretted?

 -->

## 2. About the underlying data
We used six datasets whose origin and use are described below. Further characteristics of each dataset are described in the subsections of this chapter below.

|| Origin | Used for |
|---------|-----|------|
| **Grunnkart**     |  NIBIO   | Define urban areas as the "Settlements and other artificial areas" class|
| **Administrative boundaries - Local Administrative Units**      |  European Commission    | define urban areas as Local Administrative Units (*i.e.,* municipalities) in Norway | 
| **Administrative boundaries - DEGURBA**      | European Commission    | Used to breakdown the results in different spatial units: "cities" (`DEGURBA = 1`), "towns and suburbs" (`DEGURBA = 2`), and "rural areas" (`DEGURBA = 3`) |
| **Administrative boundaries for municipalities**      |  Kartverkert    | Used to update **Administrative boundaries - Local Administrative Units** provided by the European Commission to 2024 instead of 2023. In Norway, municipalities' numbers significantly changed between these two years. In addition, Ålesund municipality was divided in two municipalities.|
| **Administrative identifiers for municipalities and fylke**| Kartverkert | Used to correct the municipalities' numbers and add Fylke number to provide another level of spatial disaggregation.|
| **Copernicus Tree Cover Density**  | Copernicus Programme - Land Monitoring Service | Calculate mean tree canopy cover density over different spatial scales.| 

<!--# Describe the data you have used in more detail, it's origin, biases, availabilit ect.-->

### 2.1 Spatial and temporal resolution and extent
|| Spatial scale | Spatial Resolution | Spatial Accuracy |
|---------|-----|------|------|
| **Grunnkart**     |  National - Norway   | MMU: 2-50m, depending on the ecosystem type mapped    | 1 to 500m depending on the dataset used to create the polygon |
| **Administrative boundaries - Local Administrative Units**      |  National - Norway    | NA | NA  |
| **Administrative boundaries - DEGURBA**      | National - Norway    | NA |  NA |
| **Administrative boundaries for municipalities**      | National - Norway    | NA |  NA |
| **Administrative identifiers for municipalities and fylke**     | National - Norway    | NA |  NA |
| **Copernicus Tree Cover Density**  | National - Norway    | 10m |  >90% | 

<!--# Describe the temporal and spatial resolution and extent of the data used -->

### 2.2 Original units
|| Original units |
|---------|-----|
| **Grunnkart**     | this dataset is a composite of multiple geospatial datasets that have been harmonised, cleaned and standardised for ecosystem accounting.  |
| **Administrative boundaries - Local Administrative Units**      | Municipalities' boundaries |
| **Administrative boundaries - DEGURBA**      | NA |
| **Administrative boundaries for municipalities**     | Municipalities' boundaries including marine sections for coastal municipalities |  
| **Administrative identifiers for municipalities and fylke**     | NA |
| **Copernicus Tree Cover Density**  | this dataset has been produced based on earth observation imagery further processed to create raster maps| 

<!--# What are the original units for the most relevant  variables in the data-->

### 2.3 Additional comments about the dataset
|| Origin | Year | Format |
|---------|-----|------|------|
| **Grunnkart**     | NIBIO   |    NA (released year: 2025) | vector |
| **Administrative boundaries - Local Administrative Units**      | European Commission    |  2023   |  vector |
| **Administrative boundaries - DEGURBA**      | European Commission     |     2024 |    excel file |
| **Administrative boundaries for municipalities**    | Kartvetkert    |    2024 |  vector |
| **Administrative identifiers for municipalities and fylke**      | Kartverkert    |     2024 |   excel file |
| **Copernicus Tree Cover Density**  | Copernicus | 2018 to 2021, yearly | raster | 
<!--# Text here -->

#### 2.3.1 Instructions for citing, using and accessing data
||Publicly accessible| Data and metadata available Available at|
|---------|-----|-----|
| **Grunnkart**     | No, upon request   |     NIBIO: https://kartkatalog.geonorge.no/metadata/grunnkart-for-bruk-i-arealregnskap-testversjon-1/28c28e3a-d88f-4a34-8c60-5efe6d56a44d. Also available locally on the NINA server (refer to the list of path in the Analyses section)|
| **Administrative boundaries - Local Administrative Units**      | Yes    | Europan Commission GISCO service: https://ec.europa.eu/eurostat/web/gisco/geodata/statistical-units/local-administrative-units. Also available locally on the NINA server (refer to the list of path in the Analyses section)|
| **Administrative boundaries - DEGURBA**      | Yes    | European Commission: https://ec.europa.eu/eurostat/web/nuts/local-administrative-units. Also available locally on the NINA server (refer to the list of path in the Analyses section) |
| **Administrative boundaries for municipalities**    | Yes    | Kartverkert: https://kartkatalog.geonorge.no/metadata/administrative-enheter-kommuner/041f1e6e-bdbc-4091-b48f-8a5990f3cc5b. Also available locally on the NINA server (refer to the list of path in the Analyses section) |
| **Administrative identifiers for municipalities and fylke**      | Yes | Kartverkert: Available from Kartverkert here:https://www.kartverket.no/til-lands/fakta-om-norge/norske-fylke-og-kommunar. Locally modified and available on the NINA server (refer to the list of path in the Analyses section)|
| **Copernicus Tree Cover Density**  | Yes | Copernicus: https://land.copernicus.eu/en/products/high-resolution-layer-forests-and-tree-cover?tab=overview. Locally modified and available on the NINA server (refer to the list of path in the Analyses section) | 


<!--# Is the data openly available? If not, how can one access it? What are the key references to the datasets?   -->


## 3. Indicator properties

### 3.1 Ecosystem Condition Typology Class (ECT)

<!--# 

Describe the rationale for assigning the indicator to the ECT class. See https://oneecosystem.pensoft.net/article/58218/
This doesnt need to be very long. Maybe just a single sentence. 

-->

### 3.2 Ecosystem condition characteristic

<!--# 

Describe the ecosystem condition characteristic represented in the indicator. See 10.3897/oneeco.6.e58218 for information on what these characteristics might be.
For example, and indicator called 'Trenching in mires' could be made to represent an ecosystem characteristic 'Intact hydrology'. The term 'characteristic' is used similar to the term 'criteria' in Multiple Criteria Decition Making.  

-->

### 3.3 Other standards

<!--# Optional: Add text about other spesific standards, e.g. national standards, and how the indicator relates to these -->

### 3.4 Collinearities with other indicators

<!--# Describe known collinearities with other metrices (indicators or variables) that could become problematic if they were also included in the same Ecosystem Condition Assessment as the indicator described here. -->


### 3.5 Impact factors

<!--# Describe the main natural and anthropogenic factors that affecst the metric -->


## 4. Reference condition and levels

### 4.1 Reference condition

<!--# Define the reference condition (or refer to where it is defined). Note the destinction between reference condition and reference levels 10.3897/oneeco.5.e58216  -->

### 4.2 Reference levels

<!--# 

If relevant (i.e. if you have normalised a variable), describe the reference levels used to normalise the variable. 

Use the terminology where X~0~ referes to the referece level (the variable value) denoting the worst possible condition; X~100~denotes the optimum or best possible condition; and X~*n*~, where in is between 0 and 100, denotes any other anchoring points linking the variable scale to the indicator scale (e.g. the threshold value between good and bad condition X~60^). 

Why was the current option chosen and how were the reference levels quantified? If the reference values are calculated as part of the analyses further down, please repeat the main information here.

 -->


#### 4.2.1 Spatial resolution and validity

<!--# 

Describe the spatial resolution of the reference levels. E.g. is it defined as a fixed value for all areas, or does it vary. Also, at what spatial scale are the reference levels valid? For example, if the reference levels have a regional resolution (varies between regions), it might mean that it is only valid and correct to use for normalising local variable values that are first aggregated to regional scale. However, sometimes the reference levels are insensitive to this and can be used to scale variables at the local (e.g. plot) scale. 

 -->

## 5. Uncertainties

<!--# Describe the main uncertainties or sources of error in the indicator or the underlying data. -->

## 6. References

<!--# You can add references manually or use a citation manager and add intext citations as with crossreferencing and hyperlinks. See https://quarto.org/docs/authoring/footnotes-and-citations.html -->

## 7. Datasets

<!--# Describe the unique datasets seperately under seperate headers (Dataset A, Dataset B, etc.-->

### 7.1 Dataset A

<!--# Describe the main dataset, typicaly the one containing the variable of (most) interest. Change the header from Dataset A to the name of the actuall dataset. -->

### 7.2. Dataset B

<!--# Describe additional datasets in a similar was as above. Deleteor add ned subheaders as needed. -->

## 8. Spatial units

<!--# 

Describe the spatial units that you rely on in your analyses. Highlight the spatial units (the resolution) that the indicator values should be interpretted at. Potential spatial delineation data should eb introduced under 7.1. Datasets. We recomend using the SEEA EA terminology opf Basic Spatial Units (BSU), Ecosystem Asses (EA) and Ecosystem Accounting Area (EAA). 

-->

## 9. Analyses

### Paths to data sources
Before starting any of the analyses, we store the path in which the data will be found. If readers wish to download the data and run this code, they can just change the paths below to their local paths for the rest of the code to function.
```{r}
#| eval: false
#| code-summary: "Paths to the data"

# DEGURBA from Eurostat
path_degurba <- "/data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO/LAU_DEGURBA_Eurostat/EU-27-LAU-2024-NUTS-2024.xlsx"

# LAUs shapefile from Eurostat
path_lau <- "/data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO/LAU_Eurostat/LAU_RG_01M_2023_3035.shp"
path_lau_folder <- "/data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO/LAU_Eurostat/"
path_lau_2024 <- "/data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO/LAU_Eurostat/LAU_EUROSTAT_2024.shp"

# Municipalities from Kartverkert
path_kom <- "/data/R/GeoSpatialData/AdministrativeUnits/Norway_AdministrativeUnits/Original/Norway_Municipalities/Administrative enheter kommuner FGDB-format/Basisdata_0000_Norge_25833_Kommuner_FGDB/Basisdata_0000_Norge_25833_Kommuner_FGDB.gdb"

# Municipalities number 2023 and 2024
path_kom_nb <- "/data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO/NO_Fylke/kommune_fylke_region_NO_2024.xlsx"

# Grunnkart
path_grunnkart <- "/data/R/GeoSpatialData/LandUse/Norway_Arealregnskap/Original/GrunnkartArealregnskap FGDB-format/"

# Path to write results
res_path <- "/data/P-Prosjekter2/412413_2023_no_egd/Results/TCCD/"

```

### Update municipal administrative boundaries from Eurostat
To ensure breakdown of the results at municipal level, we use the Eurostat Local Administrative Units shapefile from 2023 updated to 2024. The main reason behind this choice is to keep being comparable with other indicator for which this shapefile had to be used to define urban areas such as URGR, URAQ and TCCD for urban ecosystems. Another more pragmatic reason is that this shapefile is limited to the terrestrial area, while the national data source for municipal administrative boundaries include the marine territory for all coastal municipalities. As our condition indicators are limited to the terrestrial ecosystems (forest and urban ecosystems), we preferred the use of the Eurostat municipal boundaries, also called Local Administrative Units. However, this file needed to be updated manually to 2024 as the last available version was from 2023. This update was necessary as municipalities numbers in Norway underwent significant change sin 2024. Also Ålesund municipalities was divided in two. These changes are presented in the code below but the final shapefile can also be downloaded from NINA' internal repository following the path presented above `path_lau_2024`.
```{r}
#| eval: false
#| code-summary: "Update Eurotate LAUs to 2024"

# Read shapefile and select Norway
lau_no <- st_read(path_lau) %>%
           filter(CNTR_CODE == "NO")

# Read the shapefile from Kartverkert
kom_2024 <- st_read(path_kom, layer = "kommune")

# Select Haram and Ålesund municipalities
har_al_lau <- lau_no %>% 
                filter(LAU_ID == 1507)

har_al_kom <- kom_2024 %>%
                filter(kommunenummer == 1508 | kommunenummer == 1580) %>%
                st_transform(., st_crs(har_al_lau))

# Intersection between Eurostat LAU shapefile and Kartverkert 
# municipality shapefile for Haram and Ålesund municipalities 
kom_nb <- read_excel(path_kom_nb) %>%
            as.data.frame()
kom_nb$kommune_nr_2023[which(kom_nb$kommune_nr_2023 == "301")] <- "0301"

haram_2024 <- st_intersection(har_al_lau, har_al_kom[2,]) %>%
                select(c(colnames(lau_no), kommunenavn)) %>%
                select(!c(LAU_NAME)) %>%
                rename(LAU_NAME = kommunenavn) %>%
                left_join(., kom_nb, by = join_by("LAU_ID" == "kommune_nr_2023")) %>%
                select(LAU_NAME, fylke_nr, kommune_nr_2024, region_name, geometry)
haram_2024$kommune_nr_2024 <- 1580

alesund_2024 <- st_intersection(har_al_lau, har_al_kom[1,]) %>%
                select(colnames(lau_no)) %>%
                left_join(., kom_nb, by = join_by("LAU_ID" == "kommune_nr_2023")) %>%
                select(LAU_NAME, fylke_nr, kommune_nr_2024, region_name, geometry)

# Final LAUs shapefile
lau_no_2024 <- lau_no %>%
                filter(!LAU_ID == 1507) %>%
                left_join(., kom_nb, by = join_by("LAU_ID" == "kommune_nr_2023")) %>%
                select(LAU_NAME, fylke_nr, kommune_nr_2024, region_name, geometry) %>%
                rbind(., haram_2024, alesund_2024) %>%
                st_as_sf()
                
                
```

To compile the different versions of the TCCD indicators for urban ecosystems, we need to append to this shapefile the variable `DEGURBA`, which represent the degree of urbanisation of the municipality. This variable is produced at European level by Eurostat and is freely available. In the code below we join the `DEGURBA` variable to the municipality shapefile. We finish by exporting the shapefile for further analyses in the next sections.
```{r}
#| eval: false
#| code-summary: "Add the DEGURBA variable to the LAU dataset"

# Read excel file with Degurba variable and select Norway
degurba_lau_no <- read_excel(path_degurba, sheet = "NO")

degurba_lau_no$`LAU CODE` <- as.numeric(degurba_lau_no$`LAU CODE`)
degurba_lau_no$`LAU CODE`[which(degurba_lau_no$`LAU CODE` == 0301)] <- 301

# Join lau_no_2024 with DEGURBA
lau_dgurb <- inner_join(lau_no_2024,
                        degurba_lau_no,
                        by = join_by("kommune_nr_2024" == "LAU CODE")) %>%
              select(LAU_NAME, DEGURBA, fylke_nr, kommune_nr_2024, region_name, geometry) %>%
              st_as_sf()

# Export the LAUs updated shapefiles for 2024
# this is necessary for working with duckdb in the next section
st_write(lau_dgurb, paste0(path_lau_folder, "LAU_EUROSTAT_2024.shp"), append = FALSE)

# Clean the R environment
rm(list = setdiff(ls(), 
                  c("path_degurba",
                    "path_kom",
                    "path_kom_nb",
                    "path_lau",
                    "path_lau_folder",
                    "path_lau_2024",
                    "path_grunnkart",
                    "res_path",
                    "lau_dgurb")))

```

### TCCD_000 - Tree canopy cover density in forests

#### Load and prepare the tree cover density data
We prepared the Tree Canopy Cover Density rasters, they are available on the internal NINA repository indicated above. For transparency we provide the code below to download and prepare the data. Tree canopy cover density layers have been downloaded from from Copernicus for the years 2018 to 2021. This can be down using an API query through the WEkEO website (https://help.wekeo.eu/en/articles/6416936-how-to-download-wekeo-data#h_4d7de19c7c) and the `hdar` package. Note that the request through API requires the username and password from the WEkEO website. The API query uses a bounding box but it is quire large, resulting in heavy rasters of 6GB each. That is why a subsequent step of cleaning has been done to mask the cells that were not in Norway, hence decreasing the size of the rasters. These processed rasters are the one we made available on the internal NINA repository.

```{r}
#| eval: false
#| code-summary: "Load and save Copernicus Tree Cover Density data"

## Download data
# Set credentials and save them in ~/.hdarc for future use
username <- "your_username"
password <-"your_password"
client <- Client$new(username, password, save_credentials = TRUE)

# Initialise Client
client <- Client$new()

# Review and accept Terms & Condition before downloading the data
client$show_terms()
client$terms_and_conditions(term_id = 'all')

# Create API queries
# Note: use the WEkEO Data viewer that will create an automatic API
# Note: Use "SetAOI" to set your bounding box
yrs <- seq(2018, 2021, 1)

api_list <- map(seq_along(yrs), function(i){
  paste0('{
  "dataset_id": "EO:EEA:DAT:HRL:TCF",
  "product_type": "Tree Cover Density",
  "resolution": "10m",', '\n', '"year": "', as.character(yrs[i]),
 '",', '\n', '"bbox": [
    3.896921397870557,
    57.74146916326935,
    31.304658996871815,
    71.29317180330607
  ],
  "itemsPerPage": 200,
  "startIndex": 0
}'
  )
  }
) %>% 
  unlist()


# Search and dowloda the datasets
matches <- map(seq_along(api_list), ~ client$search(api_list[.x]))

output_directory <- "/data/scratch/tmp_sylvie/TCD_Copernicus/2020"

map(seq_along(matches), function(i){
  matches[[i]]$download(output_directory)
})

## Clean data
# Path the the raster data
path_tcd_raw_list <- map(seq_along(yrs),  
                         ~paste0("/data/scratch/tmp_sylvie/TCD_Copernicus/",
                                 as.character(yrs[.x]),
                                 "/"))

# Create a function to create a list of file names to read
files_list_fn <- function(path){
  files_names <- list.files(path, pattern = "\\.zip$", 
                        full.names = FALSE) %>%
                  map_vec(., ~gsub('.zip','',.))

  tcd_files <- list.files(path, pattern = "\\.zip$", 
                        full.names = TRUE)
              
  tcd_files_to_read <- map_vec(seq_along(tcd_files), function(i){paste0("/vsizip/{", tcd_files[i], "}/", files_names[i], ".tif")})
  
  return(list(tcd_files_to_read, files_names))
}


files_to_read <- map(path_tcd_raw_list, ~files_list_fn(.x))


# Create a function to read the rasters, mask Norway and export the result
process_raster_fn <- function(path_read, norway_mask, path_write, files_names){
  
  # Load libraries for parallel computing
  library(terra)
  library(sf)

  # Read
  tcd_rast <- rast(path_read)
  
  # Re-project norway boundary
  norway_mask_pj <- st_transform(norway_mask, crs = crs(tcd_rast))
  
  # Test overlap, crop and export 
  ext_tcd_rast <- ext(tcd_rast)
  ext_no <- ext(norway_mask_pj)
  test_intersect <- intersect(ext_tcd_rast, ext_no)
  if(is.null(test_intersect) == TRUE){
    return("Not in Norway")
  } else{
    # Mask
    tcd_no_mask <- crop(tcd_rast, norway_mask_pj, mask = TRUE)
  
    # Clean values 0 and 255
    rclfy_mx <- matrix(c(0, 255,
                        NaN, NaN), 
                       nrow = 2, 
                       ncol = 2)
    tcd_reclfy <- classify(tcd_no_mask, rclfy_mx)
    levels(tcd_reclfy) <- levels(tcd_rast)[[1]]
    
    # Export
    writeRaster(tcd_reclfy, paste0(path_write, files_names, "_NOcln.tif"))
    
    return("Successfully written")
    }

}

# Create Norway country boundary based on Eurostat's LAUs
no_eurostat <- st_read(path_lau_2024) %>%
                mutate(union = rep("Norway", nrow(.))) %>%
                group_by(union) %>%
                summarise()

# Create list of folder to write the final raster on NINA's internal repository
path_write_list <- map(seq_along(yrs),  
                         ~paste0("/data/P-Prosjekter2/412413_2023_no_egd/git_data/TCD/",
                                 as.character(yrs[.x]),
                                 "/"))

# Run the processing function
all_combinations <- expand.grid(seq_yrs = seq_along(files_to_read)[1], seq_files = seq_along(files_to_read[[1]][[1]]))

daemons(10)
map2(all_combinations$seq_yrs, all_combinations$seq_files,
             in_parallel(~ process_raster_fn(path_read = files_to_read[[.x]][[1]][.y],
                                norway_mask = no_eurostat,
                                path_write = path_write_list[[.x]],
                                files_names = files_to_read[[.x]][[2]][.y]),
                         process_raster_fn = process_raster_fn,
                         files_to_read = files_to_read,
                         no_eurostat= no_eurostat,
                         path_write_list = path_write_list
             )
)

daemons(0)

# Clean R environment
rm(list = setdiff(ls(), 
                  c("path_degurba",
                    "path_kom",
                    "path_kom_nb",
                    "path_lau",
                    "path_lau_folder",
                    "path_lau_2024",
                    "path_grunnkart",
                    "res_path",
                    "lau_dgurb",
                    "path_write_list")))


```

Now that we have the cleaned the rasters for the Tree Cover Density. We can create a virtual raster for each year that will be used in the subsequent analyses.
```{r}
#| eval: false
#| code-summary: "Create virtual raster of Copernicus Tree Cover Density data"

## Create a virtual raster for each year
# Create a list of the SMOD raster tiles
filename_list_vrt <- map(seq_along(path_write_list),
                          ~list.files(path = path_write_list[[.x]], pattern = '\\.tif$', 
                                 all.files = TRUE, full.names = TRUE)) 

filename_vrt_cln <- map(seq_along(filename_list_vrt),
                        ~normalizePath(filename_list_vrt[[.x]]))

# Create the virtual raster
tcd_vrt <- map(seq_along(filename_list_vrt),
               ~vrt(filename_list_vrt[[.x]]))


```

#### Load and prepare the forest data
To allow for a breakdown at municipal level, we start by intersection Eurostat Local Administrative Units with the class "Forest and woodlands" from the Grunnkart. Results at municipal level will be aggregated at national level in the Result section later on. /

Spatial intersections between municipalities and Grunnkart are computing intensive. To reduce the time, we use the `duckdb` package. Note that tile 33 of the Grunnkart, had to be cleaned separately as functions in `duckdb` couldn't make the geometry valid but functions in `sf` package could.
```{r}
#| eval: false
#| code-summary: "Forests per LAUs"
# Code by Jennifer Hansen & Sylvie Clappe

## Path to data
# Grunnkart
gdb_files <- list.files(path_grunnkart, pattern = "_gdb\\.gdb$", 
                        full.names = TRUE)

## Clean Grunnkart tile 33, 34, and 55
# Clean
gdb_issue_tiles <- gdb_files[c(7, 8, 14)]

daemons(3)
grunnkart_cln <- map(seq_along(gdb_issue_tiles), 
                     in_parallel(function(i){
                                    # Read library
                                    library(dplyr)
                                    library(sf)
  
                                    # Function
                                    grunnkart_cleaned <- st_read(gdb_issue_tiles[i], 
                                                                 query = "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Forest and woodlands'") %>%
                                                          st_transform(., crs = "EPSG:3035") %>%
                                                          st_buffer(., 0)
                                    return(grunnkart_cleaned)
                                            },
                                gdb_issue_tiles = gdb_issue_tiles)
                      )
daemons(0)

# Check
daemons(3)
grunnkart_check <- map(seq_along(grunnkart_cln), 
                       in_parallel(function(i){
                                    # Read library
                                    library(sf)
  
                                    # Function
                                    check_val <- which(st_is_valid(grunnkart_cln[[i]]) == FALSE) %>% 
                                                  length()
                                    return(check_val)
                                              },
                                    grunnkart_cln = grunnkart_cln)
                        )
daemons(0)



## Database setup
conn <- dbConnect(duckdb())
ddbs_install(conn)
ddbs_load(conn)


## Limit the number of threads (cores) duckdb uses
dbExecute(conn, "PRAGMA threads = 30")


## Register grunnkart_34 in the duckdb database
ddbs_write_vector(conn, grunnkart_cln[[1]], "grunnkart_valid_34")
ddbs_write_vector(conn, grunnkart_cln[[2]], "grunnkart_valid_33")
ddbs_write_vector(conn, grunnkart_cln[[3]], "grunnkart_valid_55")


# Check that the table was registered
ddbs_list_tables(conn)


## Write SQL queries to re-project the grunnkart and make its geometry valid
query_valid <-glue("
CREATE TABLE grunnkart_valid AS
SELECT
  okosystemtype_1,
  ST_Transform(ST_MakeValid(geo), 'EPSG:25832', 'EPSG:3035', true) AS geom
  FROM grunnkart
  WHERE okosystemtype_1 = 'Forest and woodlands'
")

query_valid_18_55 <- glue("
CREATE TABLE grunnkart_valid AS
SELECT
  okosystemtype_1,
  ST_Transform(ST_MakeValid(geo), 'EPSG:25833', 'EPSG:3035', true) AS geom
  FROM grunnkart
  WHERE okosystemtype_1 = 'Forest and woodlands'
")

query_valid_34 <- glue("
CREATE TABLE grunnkart_valid AS
SELECT
  okosystemtype_1,
  geo AS geom
  FROM grunnkart_valid_34
  WHERE okosystemtype_1 = 'Forest and woodlands'
")

query_valid_33 <- glue("
CREATE TABLE grunnkart_valid AS
SELECT
  okosystemtype_1,
  geo AS geom
  FROM grunnkart_valid_33
  WHERE okosystemtype_1 = 'Forest and woodlands'
")

query_valid_55 <- glue("
CREATE TABLE grunnkart_valid AS
SELECT
  okosystemtype_1,
  geo AS geom
  FROM grunnkart_valid_55
  WHERE okosystemtype_1 = 'Forest and woodlands'
")

query_valid_56 <- glue("
CREATE TABLE grunnkart_valid AS
SELECT
  okosystemtype_1,
  ST_Transform(ST_MakeValid(geo), 'EPSG: 25835', 'EPSG:3035', true) AS geom
  FROM grunnkart
  WHERE okosystemtype_1 = 'Forest and woodlands'
")

query_intersect <- glue("
CREATE TABLE grunnkart_intersection AS
SELECT g.*, l.*
FROM grunnkart_valid g
JOIN lau_fylke l
ON ST_Intersects(g.geom, l.geom)")

query_drop <- glue("
ALTER TABLE grunnkart_intersection
DROP COLUMN geom_1")


## Intersection

# Create function to do the spatial intersection
# includes registration of grunnkart and its cleaning
do_intersection <- function(db_connexion, fylke, path_list, fylke_index_path_list, bd_path){
  
  if(fylke %in% c(3, 11, 15, 40, 42, 46, 31, 32, 39, 50)){
    
    # Register Grunnkart for the specific fylke
    idx <- which(fylke_index_path_list == fylke)
    ar_path <- normalizePath(path_list[idx])
    ar_layer <- "arealregnskap"
    dbExecute(db_connexion, glue("
  CREATE VIEW grunnkart AS 
  SELECT *,
  FROM ST_Read('{ar_path}', layer='{ar_layer}')"))
    
    # Register LAU for specific Fylke
    lau_path <- bd_path
    dbExecute(db_connexion, glue("
  CREATE VIEW lau_fylke AS 
  SELECT *,
  FROM ST_Read('{bd_path}')"))
    
    # Spatial intersect to join the variables of both datasets before intersection
    dbExecute(db_connexion, query_valid)
    dbExecute(db_connexion, query_intersect)
    dbExecute(db_connexion, query_drop)
    
    # Perform spatial intersection
    soaa_lau <- ddbs_intersection(db_connexion, x = "grunnkart_intersection", y = "lau_fylke", crs = "EPSG:3035")
    
    # Drop the grunnkart register tables
    dbExecute(db_connexion, "DROP VIEW grunnkart")
    dbExecute(db_connexion, "DROP VIEW lau_fylke")
    dbExecute(db_connexion, "DROP TABLE grunnkart_valid")
    dbExecute(db_connexion, "DROP TABLE grunnkart_intersection")
    
    # Return the spatial intersection
    return(soaa_lau)
    
  } else if(fylke %in% c(18)){
    
    # Register grunnkart for the specific fylke
    idx <- which(fylke_index_path_list == fylke)
    ar_path <- path_list[idx]
    ar_layer <- "arealregnskap"
    dbExecute(db_connexion, glue("
  CREATE VIEW grunnkart AS 
  SELECT *,
  FROM ST_Read('{ar_path}', layer='{ar_layer}')"))
    
    # Register LAU for specific Fylke
    lau_path <- bd_path
    dbExecute(db_connexion, glue("
  CREATE VIEW lau_fylke AS 
  SELECT *,
  FROM ST_Read('{bd_path}')"))
    
    # Spatial intersect to join the variables of both datasets before intersection
    dbExecute(db_connexion, query_valid_18_55)
    dbExecute(db_connexion, query_intersect)
    dbExecute(db_connexion, query_drop)
    
    # Perform spatial intersection
    soaa_lau <- ddbs_intersection(db_connexion, x = "grunnkart_intersection", y = "lau_fylke", crs = "EPSG:3035")
    
    # Drop the grunnkart register tables
    dbExecute(db_connexion, "DROP VIEW grunnkart")
    dbExecute(db_connexion, "DROP VIEW lau_fylke")
    dbExecute(db_connexion, "DROP TABLE grunnkart_valid")
    dbExecute(db_connexion, "DROP TABLE grunnkart_intersection")
    
    # Return the spatial intersection
    return(soaa_lau)
    
  } else if(fylke == 56){
    
    # Register Grunnkart for the specific fylke
    idx <- which(fylke_index_path_list == fylke)
    ar_path <- path_list[idx]
    ar_layer <- "arealregnskap"
    dbExecute(db_connexion, glue("
  CREATE VIEW grunnkart AS 
  SELECT *,
  FROM ST_Read('{ar_path}', layer='{ar_layer}')"))
    
    # Register LAU for specific Fylke
    lau_path <- bd_path
    dbExecute(db_connexion, glue("
  CREATE VIEW lau_fylke AS 
  SELECT *,
  FROM ST_Read('{bd_path}')"))
    
    # Spatial intersect to join the variables of both datasets before intersection
    dbExecute(db_connexion, query_valid_56)
    dbExecute(db_connexion, query_intersect)
    dbExecute(db_connexion, query_drop)
    
    # Perform spatial intersection
    soaa_lau <- ddbs_intersection(db_connexion, x = "grunnkart_intersection", y = "lau_fylke", crs = "EPSG:3035")
    
    # Drop the grunnkart register tables
    dbExecute(db_connexion, "DROP VIEW grunnkart")
    dbExecute(db_connexion, "DROP VIEW lau_fylke")
    dbExecute(db_connexion, "DROP TABLE grunnkart_valid")
    dbExecute(db_connexion, "DROP TABLE grunnkart_intersection")
    
    return(soaa_lau)
    
  } else if(fylke == 34){
    
    # Register LAU for specific Fylke
    lau_path <- bd_path
    dbExecute(db_connexion, glue("
  CREATE VIEW lau_fylke AS 
  SELECT *,
  FROM ST_Read('{bd_path}')"))
    
    # Spatial intersect to join the variables of both datasets before intersection
    dbExecute(db_connexion, query_valid_34)
    dbExecute(db_connexion, query_intersect)
    dbExecute(db_connexion, query_drop)
    
    # Perform spatial intersection
    soaa_lau <- ddbs_intersection(db_connexion, x = "grunnkart_intersection", y = "lau_fylke", crs = "EPSG:3035")
    
    # Drop the grunnkart register tables
    dbExecute(db_connexion, "DROP VIEW lau_fylke")
    dbExecute(db_connexion, "DROP TABLE grunnkart_valid")
    dbExecute(db_connexion, "DROP TABLE grunnkart_intersection")
    
    return(soaa_lau)
    
  } else if(fylke == 33){

    # Register LAU for specific Fylke
    lau_path <- bd_path
    dbExecute(db_connexion, glue("
  CREATE VIEW lau_fylke AS
  SELECT *,
  FROM ST_Read('{bd_path}')"))

    # Spatial intersect to join the variables of both datasets before intersection
    dbExecute(db_connexion, query_valid_33)
    dbExecute(db_connexion, query_intersect)
    dbExecute(db_connexion, query_drop)

    # Perform spatial intersection
    soaa_lau <- ddbs_intersection(db_connexion, x = "grunnkart_intersection", y = "lau_fylke", crs = "EPSG:3035")

    # Drop the grunnkart register tables
    dbExecute(db_connexion, "DROP VIEW lau_fylke")
    dbExecute(db_connexion, "DROP TABLE grunnkart_valid")
    dbExecute(db_connexion, "DROP TABLE grunnkart_intersection")

    return(soaa_lau)
    
  } else if(fylke == 55){

    # Register LAU for specific Fylke
    lau_path <- bd_path
    dbExecute(db_connexion, glue("
  CREATE VIEW lau_fylke AS
  SELECT *,
  FROM ST_Read('{bd_path}')"))

    # Spatial intersect to join the variables of both datasets before intersection
    dbExecute(db_connexion, query_valid_55)
    dbExecute(db_connexion, query_intersect)
    dbExecute(db_connexion, query_drop)

    # Perform spatial intersection
    soaa_lau <- ddbs_intersection(db_connexion, x = "grunnkart_intersection", y = "lau_fylke", crs = "EPSG:3035")

    # Drop the grunnkart register tables
    dbExecute(db_connexion, "DROP VIEW lau_fylke")
    dbExecute(db_connexion, "DROP TABLE grunnkart_valid")
    dbExecute(db_connexion, "DROP TABLE grunnkart_intersection")

    return(soaa_lau)
    
  }else (return("an error has occured"))
  
}

# Create the input to the intersection function
numextract <- function(string){str_extract(string, "[-+]?[0-9]*\\.?[0-9]+")
}  
fylke_nb_gdb_vec <- map(seq_along(gdb_files), ~ numextract(gdb_files[.x])) %>%
                      unlist() %>%
                      as.numeric()

# Perform the intersection
forest_lau_inter <- map(seq_along(fylke_nb_gdb_vec), 
                      ~ do_intersection(conn, fylke_nb_gdb_vec[.x], gdb_files, fylke_nb_gdb_vec, path_lau_2024))

# Close the connection to duckdb database (remove in-memory dB)
dbDisconnect(conn)

# Clean the R environment
rm(list = setdiff(ls(), 
                  c("path_degurba",
                    "path_kom",
                    "path_kom_nb",
                    "path_lau",
                    "path_lau_folder",
                    "path_lau_2024",
                    "path_grunnkart",
                    "res_path",
                    "lau_dgurb",
                    "tcd_vrt",
                    "forest_lau_inter")))
```

#### Extract tree cover density percentage
Now that we have the forest area within each municipalities, we can extract the raster cells of tree cover density corresponding the forest polygons within each municipalities.




### TCCD_100 to 104 - Tree canopy cover density in urban areas


<!--# 

Use this header for documenting the analyses. Put code in seperate code chunks, and annotate the code in between using normal text (i.e. between the chunks, and try to avoid too many hashed out comments inside the code chunks). Add subheaders as needed. 

Code folding is activated, meaning the code will be hidden by default in the html (one can click to expand it).

Caching is also activated (from the top YAML), meaning that rendering to html will be quicker the second time you do it. This will create a folder inside you project folder (called INDICATORID_cache). Sometimes caching created problems because some operations are not rerun when they should be rerun. Try deleting the cash folder and try again.

-->

## 10. Results

<!--# 

Repeat the final results here. Typically this is a map or table of indicator values.

This is typically where people will harvest data from, so make sure to include all relevant output here, but don't clutter this section with too much output either.

-->

## 11. Export file

<!--# 

Optional: Display the code (don't execute it) or the workflow for exporting the indicator values to file. Ideally the indicator values are exported as a georeferenced shape or raster file with indicators values, reference values and errors. You can also chose to export the raw (un-normalised or unscaled variable) as a seperate product. You should not save large sptaial output data on GitHub. You can use eval=FALSE to avoid code from being executed (example below - delete if not relevant) 

-->

```{r export}
#| eval: false
```
