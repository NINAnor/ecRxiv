---
title: "Urban Green"
subtitle: |
  [NO_URGR_013] - Urban green in Local Administrative Units with Grunnkart, including urban green categories \
  [NO_URGR_022] - Urban green in Urban Clusters of Local Administrative Units with Grunnkart, including urban green categories  \
  [NO_URGR_023] - Urban green in the settlements of the Local Administrative Unites with Grunnkart, including urban green categories  \
  [NO_URGR_025] - Urban green in settlements defined as non-impervious areas from the settlements in Local Administrative Units with Grunnkart  \
  [NO_URGR_030] - Urban green in settlements defined as non-impervious areas from the settlements in Corine Land Cover \
  [NO_URGR_031] - Urban green in settlements defined as non-impervious areas from the settlements in Grunnkart
format: 
  html:
    embed-resources: true
    code-fold: true
    toc: true
    toc-title: Contents
    toc-depth: 3
    smooth-scroll: true
execute: 
  cache: true
author:
  - name: Sylvie Clappe              # Enter name
    email: sylvie.clappe@nina.no  # Enter email
    affiliations:
      - id: myID
        name: Norwegian Institute for Nature Research # Enter affiliations
  - name: Bálint Czúcz              
    email: balint.czucz@nina.no
    affiliations:
      - ref: myID               # To reuse affiliations referecen the id like this
date: April 22, 2025 # Enter date 
callout-icon: false
lightbox: true
css: ../../../style.css
code-links:
      - text: Add a review
        icon: github
        href: https://github.com/NINAnor/ecRxiv
        bibliography: references.bib
---

<!--# This is a template for how to document the indicator analyses. Make sure also to not change the order, or modify, the headers, unless you really need to. This is because it easier to read if all the indicators are presented using the same layout. If there is one header where you don't have anything to write, just leave the header as is, and don't write anything below it. If you are providing code, be careful to annotate and comment on every step in the analysis. Before starting it is recommended to fill in as much as you can in the metadata file. This file will populate the initial table in your output.-->

<!--# Load all you dependencies here -->

```{r setup}
#| include: false
library(knitr)
library(tidyverse)
library(kableExtra)
library(terra)
library(sf)
library(exactextractr)
library(dplyr)
library(mapview)
library(purrr)
library(readxl)
knitr::opts_chunk$set(echo = TRUE)
```

```{r source}
#| echo: false
source(here::here("_common.R"))
```

```{r}
#| echo: false
meta <- readxl::read_xlsx("../metadata.xlsx")
st <- meta |>
  filter(Variable == "status") |>
  pull(Value)
version <- meta |>
  filter(Variable == "Version") |>
  pull(Value)
auth <- meta |>
  filter(Variable == "authors") |>
  pull(Value)
year <- meta |>
  filter(Variable == "yearAdded") |>
  pull(Value)
id <- meta |>
  filter(Variable == "indicatorID") |>
  pull(Value)
name <- meta |>
  filter(Variable == "indicatorName") |>
  pull(Value)
url <- meta |>
  filter(Variable == "url") |>
  pull(Value)

meta <- meta |>
  mutate(Variable = case_match(Variable,
    "indicatorID" ~ "Indicator ID" ,
    "indicatorName" ~ "Indicator Name",
    "country" ~ "Country",
    "continent" ~ "Continent",
    "ECT" ~ "Ecosystem Condition Typology Class",
    "yearAdded" ~ "Year added",
    "yearLastUpdate" ~ "Last update",
    .default = Variable
   )
  ) |>
  filter(Variable != "authors")

```

<!--# The following parts are autogenerated. Do not edit. -->

```{r}
#| echo: false
#| results: asis
status(st)
```

::: {layout-ncol="2"}



> **Recomended citation**: `r paste(auth, " ", year, ". ", name, " (ID: ", id, ") ", "v. ", version, ". ecRxiv: https://view.nina.no/ecRxiv/", sep="")`

> **Version**: `r version`

:::

```{=html}
<details>
<summary>Show metadata</summary>
```

```{r tbl-meta}
#| tbl-cap: 'Indicator metadata'
#| echo: false
#| warning: false

meta |>
  select(Variable, Value) |>
  kbl(col.names = NULL) 

```

```{=html}
</details>
```

::: {.callout-tip collapse="true"}

## Logg

<!--# Update this logg with short messages for each update -->
- 01 Jan. 1901 - Original PR
:::


<hr />

<!--# Document you work below.  -->

## 1. Summary
***What is the ubran green index?*** \
Access to green spaces in cities are commonly called urban green spaces. They serve a lot of different functions such as contributing to people's mental and physical health, providing cool spaces in summer, as well as improving the air quality by filtrating it from certain pollutants such as particulate matters notably produced by our cars. \
This indicator is usually expressed as an average percentage of urban green space covering a city such as 10% or 80%. The higher the percentage, the more green spaces are in a city. As green spaces are mainly considered as a good thing to have in cities, this percentage indicates a certain level of quality, or *condition*. 

***How to calculate a percentage of urban green spaces in a city?*** \
There are 3 major steps in calculating this indicator:

1. Calculate the total area of cities and urban related areas. This is based on land cover data including Corine Land Cover 2018 produced by the European Environement Agency, and Grunnkart 2025 produced by Norsk Institutt for Bioøkonomi.
2. Within this area, calculate the area of urban green spaces. As previously, this is based on land cover data or the earth observation layer displaying impervious areas produced by the Copernicus program.
3. Calculate a percentage by dividing the two previous figures and multiplying the result by 100.

***How to use and interpret this indicator?*** \
There are several ways of defining areas of cities and urban green, and different datasets that can be used. That is why, in this document, we present six variations of the indicator based on three different analytical approaches. We recommend to use URGR_030 and URGR_031 as they rely on the most ecologically correct definition. They however do not follow the official guidance from the European Commission. If users need to report this indicator in an official EU reporting, we recommend to use URGR_013 or URGR_022.

## 2. About the underlying data
Due to the variations around the definition of what an urban area and an urban green areas are, we had to derive six different indicators. Their definition and associated data are described below:

1. URGR_013

***Definition***
This indicator is based on the definitions of urban and urban green areas from @cond-gn-eurostat. They can be summarised as follows:

Urban areas are defined as the Local Administrative Units (LAUs) categorised as "Cities" and their adjacent LAUs classified as "Towns and suburbs". Eurostat defines these LAUs based on the variable "DEGURBA" standing for degree of urbanisation. "Cities" correspond to `DEGURBA = 1`, while "Towns and suburbs" correspond to `DEGURBA = 2`.

Urban green areas are defined as any ecosystems that have a green cover in addition to ponds and watercourses. In their guidance (@cond-gn-eurostat) do not list precisely all the ecosystem types from the EU Ecosystem Typology that should be used. We included the following: 1.4 Urban greenspace; 1.5.2 Cemeteries; 2. Cropland; 3. Grassland; 4. Forest and woodlands; 5. Heathland and shrub; 6. Sparsely vegetated ecosystems; 7. Inland wetlands; 8. Rivers and Canals; 9. Lakes and reservoirs; 11. Coastal beaches, dunes and wetlands.

***Datasets used***
Three data sources were used:

- **Grunnkart** from NIBIO. Available from NIBIO: https://kartkatalog.geonorge.no/metadata/grunnkart-for-bruk-i-arealregnskap-testversjon-1/28c28e3a-d88f-4a34-8c60-5efe6d56a44d. Or available from NINA's internal repository: /data/R/GeoSpatialData/LandUse/Norway_Arealregnskap/Original/GrunnkartArealregnskap FGDB-format.

- **LAU** from Eurostat. Raw data available from Eurostat here: https://ec.europa.eu/eurostat/web/gisco/geodata/statistical-units/local-administrative-units. Or available from NINA's internal repository here: /data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO. Data already prepared for URGR_013 here: /data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO.

- **DEGURBA** variable for LAUs from Eurostat. Raw file available from Eurostat here: https://ec.europa.eu/eurostat/web/nuts/local-administrative-units. Or from NINA's internal repository here: /data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO

2. URGR_022

3. URGR_023

4. URGR_025

5. URGR_030

6. URGR_031

<!--# Describe the data you have used in more detail, it's origin, biases, availabilit ect.-->

### 2.1 Spatial and temporal resolution and extent

<!--# Describe the temporal and spatial resolution and extent of the data used -->

### 2.2 Original units

<!--# What are the original units for the most relevant  variables in the data-->

### 2.3 Additional comments about the dataset

<!--# Text here -->

#### 2.3.1 Instructions for citing, using and accessing data

<!--# Is the data openly available? If not, how can one access it? What are the key references to the datasets?   -->


## 3. Indicator properties

### 3.1 Ecosystem Condition Typology Class (ECT)

<!--# 

Describe the rationale for assigning the indicator to the ECT class. See https://oneecosystem.pensoft.net/article/58218/
This doesnt need to be very long. Maybe just a single sentence. 

-->

### 3.2 Ecosystem condition characteristic

<!--# 

Describe the ecosystem condition characteristic represented in the indicator. See 10.3897/oneeco.6.e58218 for information on what these characteristics might be.
For example, and indicator called 'Trenching in mires' could be made to represent an ecosystem characteristic 'Intact hydrology'. The term 'characteristic' is used similar to the term 'criteria' in Multiple Criteria Decition Making.  

-->

### 3.3 Other standards

<!--# Optional: Add text about other spesific standards, e.g. national standards, and how the indicator relates to these -->

### 3.4 Collinearities with other indicators

<!--# Describe known collinearities with other metrices (indicators or variables) that could become problematic if they were also included in the same Ecosystem Condition Assessment as the indicator described here. -->


### 3.5 Impact factors

<!--# Describe the main natural and anthropogenic factors that affecst the metric -->


## 4. Reference condition and levels

### 4.1 Reference condition

<!--# Define the reference condition (or refer to where it is defined). Note the destinction between reference condition and reference levels 10.3897/oneeco.5.e58216  -->

### 4.2 Reference levels

<!--# 

If relevant (i.e. if you have normalised a variable), describe the reference levels used to normalise the variable. 

Use the terminology where X~0~ referes to the referece level (the variable value) denoting the worst possible condition; X~100~denotes the optimum or best possible condition; and X~*n*~, where in is between 0 and 100, denotes any other anchoring points linking the variable scale to the indicator scale (e.g. the threshold value between good and bad condition X~60^). 

Why was the current option chosen and how were the reference levels quantified? If the reference values are calculated as part of the analyses further down, please repeat the main information here.

 -->


#### 4.2.1 Spatial resolution and validity

<!--# 

Describe the spatial resolution of the reference levels. E.g. is it defined as a fixed value for all areas, or does it vary. Also, at what spatial scale are the reference levels valid? For example, if the reference levels have a regional resolution (varies between regions), it might mean that it is only valid and correct to use for normalising local variable values that are first aggregated to regional scale. However, sometimes the reference levels are insensitive to this and can be used to scale variables at the local (e.g. plot) scale. 

 -->

## 5. Uncertainties

<!--# Describe the main uncertainties or sources of error in the indicator or the underlying data. -->

## 6. References

<!--# You can add references manually or use a citation manager and add intext citations as with crossreferencing and hyperlinks. See https://quarto.org/docs/authoring/footnotes-and-citations.html -->

## 7. Datasets

<!--# Describe the unique datasets seperately under seperate headers (Dataset A, Dataset B, etc.-->

### 7.1 Dataset A

<!--# Describe the main dataset, typicaly the one containing the variable of (most) interest. Change the header from Dataset A to the name of the actuall dataset. -->

### 7.2. Dataset B

<!--# Describe additional datasets in a similar was as above. Deleteor add ned subheaders as needed. -->

## 8. Spatial units

<!--# 

Describe the spatial units that you rely on in your analyses. Highlight the spatial units (the resolution) that the indicator values should be interpretted at. Potential spatial delineation data should eb introduced under 7.1. Datasets. We recomend using the SEEA EA terminology opf Basic Spatial Units (BSU), Ecosystem Asses (EA) and Ecosystem Accounting Area (EAA). 

-->

## 9. Analyses

### 9.1 URGR_013
#### 9.1.1 Preparation of the data
First we need to create separate data sets for each ecosystem types that are relevant to include in "urban green ecosystems". According to the @cond-gn-eurostat, all ecosystem types in the EU Ecosystem Types Typology with a green cover should be included. This means that, in the EU Ecosystem Typology, we only discard: 10. Marine inlets and transitional waters, 12. Marine Ecosystems,  and 1. Settlements and Other Artificial areas except classes 1.4 Urban greenspace and 1.5.2 Cemeteries. Note that the new Grunnkart has several tiles and not all of them have the same projection.
```{r}
#| eval: false
#| code-summary: "Create Ecosystem Types datasets"

# Code co-created by Sylvie Clappe and Jennifer E. Hansen
# path to the Grunnkart data
gdb_folder <- "/data/R/GeoSpatialData/LandUse/Norway_Arealregnskap/Original/GrunnkartArealregnskap FGDB-format"

# list the gdb files
gdb_files <- list.files(gdb_folder, pattern = "_gdb\\.gdb$", 
                        full.names = TRUE)

# Define CRS of the first layer (most common CRS of the gdb files)
gk_crs <- st_read(gdb_files[1], query = "SELECT * FROM arealregnskap LIMIT 500") %>%
            crs()

# Function to read a single filtered file
read_gdb <- function(gdb_path, projection_crs) {
  tryCatch({
    layer <- st_read(dsn = gdb_path, layer = "arealregnskap", query = query_string)
  }, error = function(e) {
    message("Error reading: ", gdb_path, "\n", e)
    return(NULL)  # error handling
  })
  
  if (crs(layer) == projection_crs){
    return(layer)} 
  else{
    layer_proj <- st_transform(layer, crs = projection_crs)
    print(crs(layer_proj))
    return(layer_proj)
  }
}

# Create a list of SQL queries: one per ecosystem type
query_string_list <- list()

query_string_list[[1]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_3 = 'Cemeteries'"
query_string_list[[2]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_2 = 'Urban greenspace'"
query_string_list[[3]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Lakes and reservoirs'"
query_string_list[[4]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Cropland'"
query_string_list[[5]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Grassland'"
query_string_list[[6]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Inland wetlands'"
query_string_list[[7]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Sparsely vegetated ecosystems'"
query_string_list[[8]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Rivers and canals'"
query_string_list[[9]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Coastal beaches, dunes and wetlands'"
query_string_list[[10]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Forest and woodlands'"
query_string_list[[11]] <- "SELECT * FROM arealregnskap WHERE okosystemtype_1 = 'Heathland and shrub'"


names(query_string_list) <- c("Cemeteries",
                              "Urban greenspace",
                              "Lakes and reservoirs",
                              "Cropland",
                              "Grassland",
                              "Inland wetland",
                              "Sparsely vegetated ecosystems",
                              "Rives and canals",
                              "Coastal beaches, sand dunes and wetlands",
                              "Forest and woodlands",
                              "Heathland and shrub")

# run SQL query and store results in a list
for (i in 1: length(query_string_list)){
query_string <- query_string_list[[i]] 

et_data <- gdb_files %>% 
            map(\(.) read_gdb(., projection_crs = gk_crs))  %>% 
            compact() %>%  
            bind_rows()
}

```


To compile URGR_013, we need the Norwegian Local Administrative Units (LAUs) from Eurostat. According to the @cond-gn-eurostat, only LAUs whcih are cities and the LAUs for towns and suburb adjacent to these cities should be selected. We thus used the DEGURBA variable from Eurostat to select the citites (DEGURBA = 1) and towns and surbbs (DEGURBA = 2) that were adjacent to these cities. Below are two codes: (i) one to create the dataset from scratch, and (ii) one that reads the dataset already created and available from NINA's internal repository.

```{r}
#| eval: false
#| code-summary: "Create LAUs dataset"

# Read excel file with Degurba variable and select Norway
path_lau <- "/data/P-Prosjekter2/412413_2023_no_edg/git_data/LAU_DEGURBA_Eurostat/"
degurba_lau_no <- read_excel(paste0(path_lau, "EU-27-LAU-2024-NUTS-2024.xlsx"), sheet = "NO")

# Select cities towns and suburbs
cities_sub_no <- degurba_lau_no %>%
                  select("LAU NAME NATIONAL", "DEGURBA") %>%
                  filter(DEGURBA == 1 | DEGURBA == 2) %>%
                  as.data.frame()

# Read shapefile and select Norway
path_lau <- "/data/P-Prosjekter2/412413_2023_no_edg/git_data/LAU_Eurostat/"
lau_eu <- st_read(paste0(path_lau, "LAU_RG_01M_2023_3035.shp"))
lau_no <- lau_eu %>%
           filter(CNTR_CODE == "NO")


# Join cities_sub_no with lau_no
byer_lau <- inner_join(cities_sub_no, 
                       lau_no, 
                       by = join_by("LAU NAME NATIONAL" == "LAU_NAME")) %>%
              st_as_sf()

# Select LAUs with DEGURBA == 1
lau_1 <- byer_lau %>%
          filter(DEGURBA == 1) %>%
          mutate(city_group = c(1, 2, 2, 2, 2, 2, 3, 3, 4))

# Select LAUs with DEGURBA == 2 which are adjacent to lau_1
lau_2 <- byer_lau %>%
          filter(DEGURBA == 2)

lau2_adj_id <- st_touches(lau_1, lau_2) %>%
                unlist() %>%
                unique()

lau_2_adj <- lau_2[lau2_adj_id, ]

# Create a variable to group the four cities and their suburbs
city_group <- vector()
  
for(i in 1:nrow(lau_2_adj)){
  
  if(st_touches(lau_1[1,], lau_2_adj[i,], sparse = FALSE)[,1] == TRUE){
    city_group[i] <- 1
  } 
  else if(TRUE %in% st_touches(lau_1[c(2,3,4,5,6),], lau_2_adj[i,], sparse = FALSE)[,1]){
    city_group[i] <- 2
  }
  else if(TRUE %in% st_touches(lau_1[c(7,8),], lau_2_adj[i,], sparse = FALSE)[,1]){
    city_group[i] <- 3
  }
  else if(st_touches(lau_1[9,], lau_2_adj[i,], sparse = FALSE)[,1] == TRUE){
    city_group[i] <- 4
  } else {city_group[i] <- NA}
}

# Add group variable to LAU adatset
lau_2_adj <- mutate(lau_2_adj, city_group = city_group)

# Final LAUs dataset
lau <- rbind(lau_1, lau_2_adj)

```

```{r}
#| eval: false
#| code-summary: "Upload LAUs from NINA's internal repository"

# path to the data
path_boundaries<- "/data/P-Prosjekter2/412413_2023_no_edg/git_data/LAU_URGR/"

# load the data
lau <- st_read(paste0(path_lau, "LAUs_URGR.shp"))
```


#### 9.1.2 Data Cleaning
To have correct spatial analyses, all spatial data loaded before must have the same CRS. We already ensured that the ecosystem types from the Grunnkart have the same CRS. BUt the administrative boundaries now need to be projected in the same CRS as the one from the ecosystem type.
```{r}
#| eval: false
#| code-summary: "Check CRS and reproject"

# CRS ecosystem type from Grunnkart
disc_settle_crs <- crs(et_data[[1]])

# CRS LAUs
lau_crs <- crs(lau)

# CRS regional boundaries
region_no_crs <- crs(region_no)

# Check they are the same
disc_settle_crs == region_no_crs
disc_settle_crs == lau_crs
region_no_crs == lau_crs

# Re-project boundaries
lau_32 <- st_transform(lau, disc_settle_crs)
region_no_32 <- st_transform(region_no, disc_settle_crs)
```


The next step is to check for geometrical and topological issues and fix them if there are. Note that we use the `st_make_valid()` function from the `sf` package. This function did not work for healthand and 
```{r}
#| eval: false
#| code-summary: "Geometry and topology - check and fix"

# Check for empty geometry
for(i in 1:length(et_data)){
  empty_test <- which(st_is_empty(et_data[[i]]) == TRUE)
  print(empty_test)
}

lau_32_empty <- which(st_is_empty(lau_32) == TRUE)
region_32_empty <- which(st_is_empty(region_no_32) == TRUE)

# Check validity
for(i in 1:length(et_data)){
  validity_test <- which(st_is_valid(et_data[[i]]) == FALSE)
  print(length(validity_test))
}

lau_32_invalid <- which(st_is_valid(lau_32) == FALSE)
region_32_invalid <- which(st_is_valid(region_no_32) == FALSE)

# Clean geometry of ETs with st_make_valid()
et_clean_list <- list()
for(i in c(seq(1,4,1), seq(6,8,1), 10, 11)){
  et_clean <- st_make_valid(et_data[[i]])
  et_clean_list[[i]] <- et_clean
}

et_clean_list[[5]] <- st_buffer(et_data[[5]], 0) %>%
                        filter(!st_is_empty(.)) %>%
                        st_make_valid()

et_clean_list[[9]] <- st_buffer(et_data[[9]], 0) %>%
                        filter(!st_is_empty(.)) %>%
                        st_make_valid() 

#  Check cleaning geometry worked
for(i in 1:length(et_clean_list)){
  geometry_check <- which(st_is_valid(et_clean_list[[i]]) == FALSE)
  print(length(geometry_check))
}

# Check for empty geometry
for(i in 1:length(et_clean_list)){
  empty_test <- which(st_is_empty(et_clean_list[[i]]) == TRUE)
  print(empty_test)
}

```

Now we just need to only extract polygons from the ecosystem type data. Some points and line are present. As we are interested in areas, we decide to remove them as they won't contribute to the area estimates.
```{r}
#| eval: false
#| code-summary: "Final dataset of ecosystem type"

# Keep only polygon features
et_final <- et_clean_list %>%
              map(\(.) st_collection_extract(.,
                                             type = c("POLYGON"), 
                                             warn = FALSE))
```


The data used are quite heavy, it is important to clean the R Environment to free memory space.
```{r}
#| eval: false
#| code-summary: "Clean R Environment"

# Remove non-essential R objects
rm(list=setdiff(ls(), c("et_final",
                        "region_no_32",
                        "lau_32")))

```

#### 9.1.3 Data analyses
URGR_010 is calculated at both national and regional level. Below we created one subsection for each.

##### 9.1.3.1 URGR_010 - National
First we need to calculate the areas of urban green within the LAUs and the total area of each LAUs.
```{r}
#| eval: false
#| code-summary: "Total area of ecosystem types per LAUs"
#| 
# Calculate intersection between ecosystem types and LAUs
et_lau_inter <- et_final %>%
                  map(\(.) st_intersection(., lau_32))


# Calculate areas of ecosystem types in LAUs
et_lau_area <- et_lau_inter %>%
                  map(\(.) mutate(., area_m2  = st_area(.)))

# Create a function to calculate the total area of each ecosystem type per LAUs
total_area_fn <- function(data){
  summary_tab <- data %>% 
                  st_drop_geometry() %>%
                  select(okosystemtype_1, okosystemtype_2, area_m2, LAU_ID) %>%             
                  group_by(LAU_ID) %>%
                  summarise(total_area = sum(area_m2))
  return(summary_tab)
}


# Calculate total area of each ecosystem types per LAUs
et_lau_tot_area <- et_lau_area %>%
                    map(total_area_fn)

# Sum the area of all ecosystem types per LAUs
area_lau_green_ets <- et_lau_tot_area %>%
                        map(\(.) sum(.$total_area))

# Calculate total area of all LAUs            
area_lau <- lau_32 %>%
              mutate(area_m2 = st_area(.)) %>%
              st_drop_geometry() %>%
              group_by(LAU_ID) %>%
              summarise(total_area = sum(area_m2))





```

Now, based on the areas calculated before, we can compile URGR_010 at national level.
```{r}
#| eval: false
#| code-summary: "Calculate URGR_010 - National"

urgur_010_national <- sum(unlist(area_green_ets)) / sum(area_lau) * 100

```


##### 9.1.3.2 URGR_010 - Regional
The process is very similar to what has been done to compile URGR_010 at national level. We will be using some of the areas we calculated before. The first step is however to associate the LAUs to each region. For this we need to do a spatial intersection.

```{r}
#| eval: false
#| code-summary: "Connect LAUs and regions"

# Calculate Intersection LAUs and Regions
lau_region_inter <- st_intersection(lau_32, region_no_32)


```

Once this is done, we first have to prouce a summary table of the total area of ecosystem types per LAUs.
```{r}
#| eval: false
#| code-summary: "Summary table of total area of ecosystem types per LAUs"

# Create summary table with areas of all ETs per LAUs
et_area_lau_summary <- et_lau_tot_area %>%
                        compact() %>%
                        bind_rows() %>%
                        group_by(LAU_ID) %>%
                        summarise(total_area = sum(total_area))
```

Then we can join this table with the intersection spatial layer to add the information on region names. This gives the total area of ecosystem types present in LAUs for each regions. In parallel we also calculate the total area of LAUs per regions.

```{r}
#| eval: false
#| code-summary: "Total areas of ecosystem types and LAUs per regions"

# Total area of ecosystem types in the LAUs of each region 
green_area_rgn <- lau_region_inter %>%
                    st_drop_geometry %>%
                    select(LAU_ID, regn_nm) %>%
                    inner_join(., et_area_lau_summary, by = "LAU_ID") %>%
                    group_by(regn_nm) %>%
                    summarise(total_green_area = sum(total_area))

# Total area of LAUs in ecah region             
lau_area_rgn <- lau_region_inter %>%
                  st_drop_geometry %>%
                  select(LAU_ID, regn_nm) %>%
                  inner_join(., area_lau, by = "LAU_ID") %>%
                  group_by(regn_nm) %>%
                  summarise(total_area_lau_rgn = sum(total_area))
```

Finally, we can use all the areas calculated above to compile a regional URGR_010.

```{r}
#| eval: false
#| code-summary: "Compile URGR_010"

# URGR_010 - regional
urgur_010_rgn <- lau_area_rgn %>% 
                  inner_join(., green_area_rgn, by = "regn_nm") %>%
                  mutate(urgr_010 = total_green_area /  total_area_lau_rgn * 100)
```

### 9.2 URGR_11 - Corine Land Cover
The workflow is very similar to the previous URGR_010 calculation with the Grunnkart.

#### 9.2.1 Load and prepare the data
We need Corine Land Cover 2018, form which we select the same ecosystem types as before.
```{r}
#| eval: false
#| code-summary: "Upload and prepare Corine Land Cover"

# Load Corine Land Cover 2018
path_clc <- "/data/R/GeoSpatialData/LandCover/Norway_CORINE_Landcover/Original/CORINE_2018/"
clc18 <- st_read(paste0(path_clc, "0000_32632_corine2018_8d4abc_SHAPE.shp"))

# Select the ecosystem types of interest
clc18_green <- clc18 %>%
                  filter(! clc18_kode == 111,
                         ! clc18_kode == 121,
                         ! clc18_kode == 122,
                         ! clc18_kode == 123,
                         ! clc18_kode == 131,
                         ! clc18_kode == 132,
                         ! clc18_kode == 133,
                         ! clc18_kode == 521,
                         ! clc18_kode == 522,
                         ! clc18_kode == 523)
```

Same as for URGR_010, we then upload the boundaries of LAUs and regions.
```{r}
#| eval: false
#| code-summary: "Upload Administrative boundaries"

# path to the data
path_boundaries<- "/data/P-Prosjekter2/412413_2023_no_edg/git_data/"

# load the data
lau <- st_read(paste0(path_lau, "LAUs_URGR_010.shp"))
region_no <- st_read(paste0(path_boundaries, "Norway_Regions_2024.shp"))

```

#### 9.2.2 Data cleaning
We decided to use the same Coordiante Reference System (CRS) than used for URGR_010. So we decided to project Corine Land Cover and the administrative boundaries in the grunnkart projection. We also checked for geometrical and topological issues.
```{r}
#| eval: false
#| code-summary: "Projection"

# CRS Grunnkart
# File path to Grunnkart
gdb_folder <- "/data/R/GeoSpatialData/LandUse/Norway_Arealregnskap/Original/GrunnkartArealregnskap FGDB-format"

# Pattern for stored gdb
gdb_files <- list.files(gdb_folder, pattern = "_gdb\\.gdb$", 
                        full.names = TRUE)

# Define CRS of the first layer (most common CRS of the gdb files)
gk_crs <- st_read(gdb_files[1]) %>%
            crs()

# CLC CRS
clc18_green_crs <- crs(clc18_green)

# LAUs
lau_crs <- crs(lau)

# Region and fylke boundaries
region_no_crs <- crs(region_no)
fylke_no_crs <- crs(fylke_no)

# Check they are the same
clc18_green_crs == gk_crs
clc18_green_crs == region_no_crs
clc18_green_crs == lau_crs
region_no_crs == lau_crs
gk_crs == lau_crs
region_no_crs == gk_crs


# Re-project boundaries (they are less heavy!)
clc18_green_32 <- st_transform(clc18_green, gk_crs)
lau_32 <- st_transform(lau, gk_crs)
region_no_32 <- st_transform(region_no, gk_crs)
```

```{r}
#| eval: false
#| code-summary: "Geometry and topology check"

# Check for empty geometry
clc18_green_empty <- which(st_is_empty(clc18_green_32) == TRUE)
lau_32_empty <- which(st_is_empty(lau_32) == TRUE)
region_32_empty <- which(st_is_empty(region_no_32) == TRUE)

# Check validity
clc18_green_invalid <- which(st_is_valid(clc18_green_32) == FALSE)
lau_32_invalid <- which(st_is_valid(lau_32) == FALSE)
region_32_invalid <- which(st_is_valid(region_no_32) == FALSE)
```

Then we clean our R environment before the analyses.
```{r}
#| eval: false
#| code-summary: "Clean R environment"

# Remove non-essential R objects
rm(list=setdiff(ls(), c("clc18_green_32",
                        "region_no_32",
                        "lau_32")))
```

#### 9.2.3 Data analyses
##### URGR_011 - National
First we need to do an intersection between Corine Land Cover and the LAUs administrative boundaries.
```{r}
#| eval: false
#| code-summary: "Intersecting Corine Land Cover and LAUs"

# Calculate intersection between ETs and LAUs
et_lau_inter <-  st_intersection(clc18_green_32, lau_32)
```

Then we calculate the areas of green ecosystem types per LAUs and the total areas of the LAUs.
```{r}
#| eval: false
#| code-summary: "Calculate areas"

# Calculate areas of ETs in LAUs
et_lau_area <- et_lau_inter %>%
                  mutate(., area_m2  = st_area(.))


# Calculate areas of ETs in LAUs
et_lau_tot_area <- et_lau_area %>% 
                    st_drop_geometry() %>%
                    select(clc18_kode, area_m2, LAU_ID) %>%             
                    group_by(LAU_ID) %>%
                    summarise(total_area = sum(area_m2))

#Calculate the total areas of ecosystem types within LAUs
area_lau_green_ets <- et_lau_tot_area$total_area %>%
                        unclass() %>%
                        sum()
              

# Calculate the total areas of the LAUs           
area_lau <- lau_32 %>%
              mutate(area_m2 = st_area(.)) %>%
              st_drop_geometry() %>%
              group_by(LAU_ID) %>%
              summarise(total_area = sum(area_m2))
```

Once we have the areas, the URGR_011 index can be calculated as per (total area of green ecosystem types within LAUs)/(total areas of the LAUs) * 100.
```{r}
#| eval: false
#| code-summary: "Compile National URGR_011"

urgr_011_nat <- sum(area_lau_green_ets) / sum(unclass(area_lau$total_area)) * 100
```

##### URGR_011 - Regional
The first step is to do an intersection between the LAUs and the regions to attribute the LAUs to a region.
```{r}
#| eval: false
#| code-summary: "Intersecting LAUs and regions"

# Calculate Intersection LAUs and Regions
lau_region_inter <- st_intersection(lau_32, region_no_32)
```

From there, we can just join this new layer with the areas calculated before so the region information is brought along and attributed to LAUs total areas and total areas of green ecosystem types per LAUs.
```{r}
#| eval: false
#| code-summary: "Toral areas of green ecosystem types and LAUs per regions"

# Join Regions and LAUs with 
green_area_rgn <- lau_region_inter %>%
                    st_drop_geometry %>%
                    select(LAU_ID, regn_nm) %>%
                    inner_join(., et_lau_tot_area, by = "LAU_ID") %>%
                    group_by(regn_nm) %>%
                    summarise(total_green_area = sum(total_area))
              
lau_area_rgn <- lau_region_inter %>%
                  st_drop_geometry %>%
                  select(LAU_ID, regn_nm) %>%
                  inner_join(., area_lau, by = "LAU_ID") %>%
                  group_by(regn_nm) %>%
                  summarise(total_area_lau_rgn = sum(total_area))
```

We can then calculate URGR_011 at regional level, the same way we calculated it at national level.
```{r}
#| eval: false
#| code-summary: "Compile Regional URGR_011"

urgr_011_rgn <- lau_area_rgn %>% 
                  inner_join(., green_area_rgn, by = "regn_nm") %>%
                  mutate(urgr_010 = total_green_area /  total_area_lau_rgn * 100)
```


<!--# 

Use this header for documenting the analyses. Put code in separate code chunks, and annotate the code in between using normal text (i.e. between the chunks, and try to avoid too many hashed out comments inside the code chunks). Add subheaders as needed. 

Code folding is activated, meaning the code will be hidden by default in the html (one can click to expand it).

Caching is also activated (from the top YAML), meaning that rendering to html will be quicker the second time you do it. This will create a folder inside you project folder (called INDICATORID_cache). Sometimes caching created problems because some operations are not rerun when they should be rerun. Try deleting the cash folder and try again.

-->

## 10. Results

<!--# 

Repeat the final results here. Typically this is a map or table of indicator values.

This is typically where people will harvest data from, so make sure to include all relevant output here, but don't clutter this section with too much output either.

-->

## 11. Export file

<!--# 

Optional: Display the code (don't execute it) or the workflow for exporting the indicator values to file. Ideally the indicator values are exported as a georeferenced shape or raster file with indicators values, reference values and errors. You can also chose to export the raw (un-normalised or unscaled variable) as a seperate product. You should not save large sptaial output data on GitHub. You can use eval=FALSE to avoid code from being executed (example below - delete if not relevant) 

-->

```{r export}
#| eval: false
```
